# RAG Agent PM â€“ Semantic Search Agent

## Overview

rag-agent-pm is a Retrieval-Augmented Generation (RAG) agent optimized for semantic search across Alleatoâ€™s project management documents and meetings. It enables fast, intelligent retrieval of both conceptual queries and specific facts, with support for multiple LLM providers and flexible deployment options.

â¸»

## ğŸ”‘ Key Features
	â€¢	Dual Search Strategies
	â€¢	Semantic search for conceptual queries
	â€¢	Hybrid search for fact-based queries (vector + text search)
	â€¢	Vector Similarity: PostgreSQL + PGVector
	â€¢	Multi-Provider LLM Support: OpenAI, Anthropic, Groq, Ollama, and other OpenAI-compatible APIs
	â€¢	Streaming Responses: Real-time token streaming in CLI and API
	â€¢	Conversation Memory: Context persistence with message scoring and compression
	â€¢	FastAPI Backend: RESTful API with CORS support for browser apps

â¸»

## ğŸ—ï¸ Architecture
	1.	Agent Core (agent.py) â€“ Pydantic-based agent with semantic + hybrid search tools
	2.	CLI Interface (cli.py) â€“ Rich terminal UI with streaming responses
	3.	FastAPI Server (app.py) â€“ Production-ready API with streaming endpoints
	4.	Search Tools â€“ Automatic selection between semantic vs. hybrid
	5.	Document Processing â€“ Intelligent chunking + embeddings stored in PostgreSQL

â¸»

## âš™ï¸ Setup

Prerequisites
	â€¢	Python 3.10+
	â€¢	PostgreSQL with PGVector extension
	â€¢	At least one LLM API key (OpenAI, Anthropic, etc.)

Installation

1. Install dependencies
pip install -r requirements.txt

2. Initialize database schema
psql -d your_database -f sql/schema.sql

3. Configure environment
export DATABASE_URL=postgresql://user:password@host:5432/database
export LLM_API_KEY=your-api-key
export LLM_PROVIDER=openai
export LLM_MODEL=gpt-4-1106-preview
export EMBEDDING_MODEL=text-embedding-3-small

4. Ingest documents
python -m ingestion.ingest --documents documents/


â¸»

## ğŸš€ Usage

CLI Mode

python -m cli

	â€¢	Interactive terminal with live streaming
	â€¢	Shows tool execution + retrieved context

API Server

uvicorn app:app --reload

Endpoints:
	â€¢	POST /chat â†’ non-streaming chat
	â€¢	POST /chat/stream â†’ streaming (SSE) chat
	â€¢	GET /health â†’ health check

â¸»

## ğŸ“‚ Document Context

The /documents/ directory stores Alleatoâ€™s operational knowledge base:
	â€¢	Weekly ops meetings
	â€¢	Client calls (Uniqlo, Goodwill, etc.)
	â€¢	Project reviews and design sessions
	â€¢	Business development activities
	â€¢	Employee onboarding/interviews

All documents are automatically chunked, embedded, and stored for retrieval.

â¸»

âš¡ Triggers
	1.	CLI â€“ python -m cli
	â€¢	Streaming Q&A with history persistence
	2.	API â€“ uvicorn app:app
	â€¢	POST /chat, POST /chat/stream, GET /health
	3.	Document Ingestion â€“

python -m ingestion.ingest --documents documents/ --clean

	â€¢	Parses Markdown â†’ splits into chunks â†’ generates embeddings â†’ stores in PostgreSQL

â¸»

## ğŸ—„ï¸ Database Schema (Supabase/Postgres)

Document Storage

documents
	â€¢	id UUID PK
	â€¢	title TEXT
	â€¢	source TEXT
	â€¢	content TEXT
	â€¢	metadata JSONB
	â€¢	created_at/updated_at TIMESTAMPTZ

chunks
	â€¢	id UUID PK
	â€¢	document_id UUID FK
	â€¢	content TEXT
	â€¢	embedding VECTOR(1536)
	â€¢	chunk_index INT
	â€¢	metadata JSONB
	â€¢	token_count INT
	â€¢	created_at TIMESTAMPTZ

Conversation Memory

conversations â€“ session-level context
conversation_messages â€“ per-message memory + embeddings
conversation_facts â€“ extracted entities, goals, constraints
conversation_retrievals â€“ search results + relevance scores

â¸»

## ğŸ¯ RAG Strategy

1. Semantic Search
	â€¢	Use case: conceptual queries
	â€¢	Method: pure vector similarity (cosine distance)
	â€¢	Example: â€œideas about client meetingsâ€

2. Hybrid Search
	â€¢	Use case: exact facts / names / dates
	â€¢	Method: vector + PostgreSQL full-text search
	â€¢	Scoring: (vector_sim Ã— (1-text_weight)) + (text_sim Ã— text_weight)
	â€¢	Example: â€œUniqlo meeting on March 15thâ€

â¸»

ğŸ§  Advanced Features
	â€¢	Memory Management
	â€¢	Importance scoring
	â€¢	Automatic compression of old messages
	â€¢	Query Enhancement
	â€¢	HyDE (Hypothetical Document Embeddings)
	â€¢	Sub-query generation for complex questions
	â€¢	Result Reranking
	â€¢	Cross-encoder reranking
	â€¢	MMR for diversity
	â€¢	Recency boost

â¸»

## ğŸ”„ Data Flow

User Query â†’ Agent â†’ Strategy Selection
           â†’ Database Query â†’ Embedding Generation
           â†’ Search Execution â†’ Result Retrieval
           â†’ LLM Processing â†’ Streaming Response


â¸»

ğŸ“ Document Ingestion Flow

Markdown Files â†’ Content Extraction
               â†’ Intelligent Chunking
               â†’ Embedding Generation
               â†’ PostgreSQL Storage (PGVector)
               â†’ Vector Index Creation


â¸»

âœ… Key Config

DATABASE_URL=postgresql://user:password@host:5432/database
LLM_API_KEY=your-openai-key
LLM_PROVIDER=openai
LLM_MODEL=gpt-4-1106-preview
EMBEDDING_MODEL=text-embedding-3-small


â¸»

This RAG agent is tailored for Alleatoâ€™s project management workflows, enabling teams to quickly search meeting history and documents with semantic + hybrid retrieval strategies.